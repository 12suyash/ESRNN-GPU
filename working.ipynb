{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODOS\n",
    "\n",
    "# implement gradient clipping \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_config = {\n",
    "    \n",
    "#     RUNTIME PARAMETERS\n",
    "    'prod': False,\n",
    "    'device': (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \n",
    "    'lback': False,\n",
    "    \n",
    "#     MODEL STRUCTURE PARAMETER\n",
    "    \n",
    "    \n",
    "    'variable': \"Quarterly\",\n",
    "    'run': \"50/45 (1,2),(4,8), LR=0.001/{10,1e-4f}, EPOCHS=15, LVP=80 40*\",\n",
    "    'percentile': 50,\n",
    "    'training_percentile': 45,\n",
    "    'dilations': ((1, 2), (4, 8)),\n",
    "    'use_residual_lstm': False,\n",
    "    'add_nl_layer': False,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'learning_rates': ((10, 1e-4)),\n",
    "    'per_series_lr_multip': 1,\n",
    "    'num_of_train_epochs': 15,\n",
    "    'state_hsize': 40,\n",
    "    'seasonality': 4,\n",
    "    'input_size': 4,\n",
    "    'output_size': 8,\n",
    "    'min_inp_seq_len': 0,\n",
    "    'level_variability_penalty': 80,\n",
    "    'batch_size': 8\n",
    "    \n",
    "    'num_of_categories': 6, # in data provided\n",
    "    'big_loop': 3,\n",
    "    'num_of_chunks': 2,\n",
    "    'eps': 1e-6,\n",
    "    'averaging_level': 5,\n",
    "    'use_median': False,\n",
    "    'middle_pos_for_avg': 2, # if using medians\n",
    "    'noise_std'=0.001, \n",
    "    'freq_of_test': 1,\n",
    "    'gradient_clipping': 20,\n",
    "    'c_state_penalty': 0,\n",
    "    'big_float': 1e38, # numeric_limits<float>::max(),\n",
    "    'print_diagn': True,\n",
    "    'max_num_of_series': -1,\n",
    "    \n",
    "    'use_auto_learning_rate': False,\n",
    "    'min_learning_rate': 0.0001f,\n",
    "    'lr_ratio': sqrt(10),\n",
    "    'lr_tolerance_multip': 1.005,\n",
    "    'l3_period': 2,\n",
    "    'min_epochs_before_changing_lrate': 2,\n",
    "    'print_train_batch_every': 5\n",
    "}\n",
    "\n",
    "q_config['input_size_i'] = q_config['input_size']\n",
    "q_config['output_size_i'] = q_config['output_size']\n",
    "q_config['min_series_length'] = q_config['input_size_i'] + q_config['output_size_i'] + q_config['min_inp_seq_len'] + 2\n",
    "q_config['max_series_length'] = 40 * q_config['seasonality'] + q_config['min_series_length']\n",
    "q_config['tau']: q_config['percentile'] / 100\n",
    "q_config['training_tau']: q_config['training_percentile'] / 100\n",
    "q_config['attention_hsize']: q_config['state_hsize']\n",
    "    \n",
    "if not q_config['prod']:\n",
    "    q_config['batch_size'] = 2\n",
    "    q_config['max_num_of_series'] = 40\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv('./data/info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_location):\n",
    "    series = []\n",
    "    ids = []\n",
    "    with open(file_location, 'r') as file:\n",
    "        data = file.read().split(\"\\n\")\n",
    "\n",
    "    for i in range(1, len(data)-1):\n",
    "        row = data[i].replace('\"', '').split(',')\n",
    "        series.append(np.array([float(j) for j in row[1:] if j != \"\"]))\n",
    "        ids.append(row[0])\n",
    "\n",
    "    series = np.array(series)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_val_set(train, output_size):\n",
    "    val = []\n",
    "    for i in range(len(train)):\n",
    "        val.append(train[i][-output_size:])\n",
    "        train[i] = train[i][:-output_size]\n",
    "    return np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(train_file_location, test_file_location, output_size):\n",
    "    train = read_file(train_file_location)\n",
    "    test = read_file(test_file_location)\n",
    "    vals = create_val_set(train, output_size)\n",
    "    return train, vals, test        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_datasets('./data/M4DataSetTrain/Quarterly-train.csv', \n",
    "                                   './data/M4DataSetTest/Quarterly-test.csv', \n",
    "                                   q_config['output_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataTrain, dataVal, dataTest, info, variable, device):\n",
    "        self.dataInfoCatOHE = pd.get_dummies(info[info['SP'] == variable]['category'])\n",
    "        self.dataInfoCatHeaders = self.dataInfoCatOHE.columns\n",
    "        self.dataInfoCat = torch.from_numpy(self.dataInfoCatOHE.values)\n",
    "        self.dataTrain = [torch.tensor(i) for i in dataTrain]   \n",
    "        self.dataVal = [torch.tensor(i) for i in dataVal]\n",
    "        self.dataTest = [torch.tensor(i) for i in dataTest] \n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataTrain)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataTrain[idx].to(self.device), \\\n",
    "                self.dataVal[idx].to(self.device), \\\n",
    "                self.dataTest[idx].to(self.device), \\\n",
    "                self.dataInfoCat[idx].to(self.device), \\\n",
    "                idx\n",
    "\n",
    "def collate_lines(seq_list):\n",
    "    train_, val_, test_, info_cat_, idx_ = zip(*seq_list)\n",
    "    train_lens = [len(seq) for seq in train_]\n",
    "    seq_order = sorted(range(len(train_lens)), key=train_lens.__getitem__, reverse=True)\n",
    "    train = [train_[i] for i in seq_order]\n",
    "    val = [val_[i] for i in seq_order]\n",
    "    test = [test_[i] for i in seq_order]\n",
    "    info_cat = [info_cat_[i] for i in seq_order]\n",
    "    idx = [idx_[i] for i in seq_order]\n",
    "    return train, val, test, info_cat, idx\n",
    "\n",
    "dataset = SeriesDataset(train, val, test, info, q_config['variable'], q_config['device'])\n",
    "dataloader = DataLoader(dataset, batch_size=q_config['batch_size'], shuffle=True, collate_fn=collate_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESRNN(nn.Module):\n",
    "    def __init__(self, num_series, seasonality, hidden_size, output_size):\n",
    "        super(ESRNN, self).__init__()\n",
    "        init_lev_sms = []\n",
    "        init_seas_sms = []\n",
    "        init_seasonalities = []\n",
    "        \n",
    "        # NEED TO ENSURE THAT THE GRADIENTS ARE ACCRUEING, IF NOT NEED TO TRY CREATING THESE PARAMETERS AS VARIABLES\n",
    "        # ANOTHER THING TO LOOK AT IS RATHER THAN INDEXING NORMALLY TO USE INDEX_SELECT METHOD ON THE TENSOR\n",
    "#             UPDATE 2018-11-30: PARAMETERS SHOWING IN MODEL PRINT (AREDD)\n",
    "        for i in range(num_series):\n",
    "            init_lev_sms.append(nn.Parameter(torch.Tensor([0.5])))\n",
    "            init_seas_sms.append(nn.Parameter(torch.Tensor([0.5])))\n",
    "            init_seasonalities.append(nn.Parameter(torch.ones(seasonality) * 0.5))\n",
    "        \n",
    "        self.init_lev_sms = nn.ParameterList(init_lev_sms)\n",
    "        self.init_seas_sms = nn.ParameterList(init_seas_sms)\n",
    "        self.init_seasonalities = nn.ParameterList(init_seasonalities)\n",
    "        \n",
    "        self.nl_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.Tanh()\n",
    "        self.scoring = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, train, val, test, info_cat, idxs, add_nl_layer):\n",
    "#         GET THE PER SERIES PARAMETERS\n",
    "        lev_sms = torch.stack([self.init_lev_sms[idx] for idx in idxs])\n",
    "        seas_sms = torch.stack([self.init_seas_sms[idx] for idx in idxs])\n",
    "        seasonalities = torch.stack([self.init_seasonalities[idx] for idx in idxs])\n",
    "\n",
    "        \n",
    "#         WINDOWING LOOP\n",
    "#         TIME LOOP\n",
    "#         RNN STUFF HERE\n",
    "        \n",
    "#         if add_nl_layer:\n",
    "#             out = self.nl_layer(out)\n",
    "#             out = self.act(out)\n",
    "#         out = self.scoring(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ESRNNTrainer(nn.Module):\n",
    "    def __init__(self, model, dataloader, run_id):\n",
    "        super(ESRNNTrainer, self).__init__()\n",
    "        self.model = model.to(config['device'])\n",
    "        self.dl = dataloader\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'], eps=q_config['eps'])\n",
    "        self.criterion = None\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = config['num_epochs']\n",
    "        self.run_id = str(run_id)\n",
    "        self.prod_str = 'prod' if config['prod'] else 'dev'               \n",
    "        self.log = Logger(\"./logs/train%s%s\" % (self.prod_str, self.run_id))   \n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (train, val, test, info_cat, idx) in enumerate(self.dl):\n",
    "            if batch_num % q_config['print_train_batch_every'] == 0:\n",
    "                print(\"train_batch: %d\" % batch_num)\n",
    "            loss = self.train_batch(inputs, targets_inputs, targets_output, target_lens)\n",
    "            epoch_loss += loss\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        info = {'loss': epoch_loss}\n",
    "        for tag, value in info.items():\n",
    "            self.log.log_scalar(tag, value, self.epochs + 1)    \n",
    "        return epoch_loss\n",
    "\n",
    "    def train_batch(self, train, val, test, info_cat, idx):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(train, val, test, info_cat, idx)\n",
    "        loss = self.criterion(output, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinball Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expression pinBallLoss(const Expression& out_ex, const Expression& actuals_ex) {//used by Dynet, learning loss function\n",
    "#   vector<Expression> losses;\n",
    "#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n",
    "#     auto forec = pick(out_ex, indx);\n",
    "#     auto actual = pick(actuals_ex, indx);\n",
    "#     if (as_scalar(actual.value()) > as_scalar(forec.value()))\n",
    "#       losses.push_back((actual - forec)*TRAINING_TAU);\n",
    "#     else\n",
    "#       losses.push_back((actual - forec)*(TRAINING_TAU - 1));\n",
    "#   }\n",
    "#   return sum(losses) / OUTPUT_SIZE * 2;\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     as defined in the blog post --- https://eng.uber.com/m4-forecasting-competition/\n",
    "class PinballLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, training_tau, output_size):\n",
    "        super(PinballLoss, self).__init__()\n",
    "        self.training_tau = training_tau\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def forward(self, predictions, actuals):\n",
    "        losses = []\n",
    "        for i in range(self.output_size):\n",
    "            prediction = predictions[i]\n",
    "            actual = actuals[i]\n",
    "            if actual > prediction:\n",
    "                losses.append((actual - prediction) * self.training_tau)\n",
    "            else:\n",
    "                losses.append((actual - prediction) * (self.training_tau - 1))\n",
    "        loss = torch.Tensor(losses)\n",
    "        return torch.sum(loss) / self.output_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2912)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = torch.rand(100)\n",
    "test2 = torch.rand(100)\n",
    "pb = PinballLoss(0.5, 100)\n",
    "pb(test1, test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sMAPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float sMAPE(vector<float>& out_vect, vector<float>& actuals_vect) {\n",
    "#   float sumf = 0;\n",
    "#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n",
    "#     auto forec = out_vect[indx];\n",
    "#     auto actual = actuals_vect[indx];\n",
    "#     sumf+=abs(forec-actual)/(abs(forec)+abs(actual));\n",
    "#   }\n",
    "#   return sumf / OUTPUT_SIZE * 200;\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sMAPE(predictions, actuals, output_size):\n",
    "    sumf = 0\n",
    "    for i in range(output_size):\n",
    "        prediction = predictions[i]\n",
    "        actual = actuals[i]\n",
    "        sumf += abs(prediction - actual) / (abs(prediction) + abs(actual))\n",
    "        \n",
    "    return sumf / output_size * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(76.0139)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = torch.rand(100)\n",
    "test2 = torch.rand(100)\n",
    "sMAPE(test1, test2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wQuantLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float wQuantLoss(vector<float>& out_vect, vector<float>& actuals_vect) {\n",
    "#   float sumf = 0; float suma=0;\n",
    "#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n",
    "#     auto forec = out_vect[indx];\n",
    "#     auto actual = actuals_vect[indx];\n",
    "#     suma+= abs(actual);\n",
    "#     if (actual > forec)\n",
    "#       sumf = sumf + (actual - forec)*TAU;\n",
    "#     else\n",
    "#       sumf = sumf + (actual - forec)*(TAU - 1);\n",
    "#   }\n",
    "#   return sumf / suma * 200;\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wQuantLoss(predictions, actuals, output_size, training_tau):\n",
    "    sumf = 0\n",
    "    suma = 0\n",
    "    for i in range(output_size):\n",
    "        prediction = predictions[i]\n",
    "        actual = actuals[i]\n",
    "        \n",
    "        suma += abs(actual)\n",
    "        if (actual > prediction):\n",
    "            sumf = sumf + (actual - prediction) * training_tau\n",
    "        else:\n",
    "            sumf = sumf + (actual - prediction) * (training_tau - 1)\n",
    "            \n",
    "    return sumf / suma * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(70.5309)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = torch.rand(100)\n",
    "test2 = torch.rand(100)\n",
    "wQuantLoss(test1, test2, 100, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ErrorFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float errorFunc(vector<float>& out_vect, vector<float>& actuals_vect) {\n",
    "#   if (PERCENTILE==50)\n",
    "#     return sMAPE(out_vect, actuals_vect);\n",
    "#   else\n",
    "#     return wQuantLoss(out_vect, actuals_vect);\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def errorFunc(predictions, actuals, output_size, percentile):\n",
    "    if (percentile == 50):\n",
    "        return sMAPE(predictions, actuals, output_size)\n",
    "    else:\n",
    "        return wQuantLoss(predictions, actuals, output_size, percentile / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.5585)\n",
      "tensor(55.5585)\n",
      "tensor(68.8411)\n",
      "tensor(68.8411)\n"
     ]
    }
   ],
   "source": [
    "test1 = torch.rand(100)\n",
    "test2 = torch.rand(100)\n",
    "print(errorFunc(test1, test2, 100, 48))\n",
    "print(wQuantLoss(test1, test2, 100, 0.48))\n",
    "\n",
    "print(errorFunc(test1, test2, 100, 50))\n",
    "print(sMAPE(test1, test2, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p36",
   "language": "python",
   "name": "pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
